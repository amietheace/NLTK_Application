{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Noise Removal\n",
    "\n",
    "Any piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n",
    "\n",
    "A general approach for noise removal is to prepare a dictionary of noisy entities, and iterate the text object by tokens(or by words), eliminating those tokens which are present in noise dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my text'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_list = [\"is\", \"a\", \"this\", \"...\"]\n",
    "\n",
    "def _remove_noise(input_text):\n",
    "    words = input_text.split()\n",
    "    noise_free_words = [word for word in words if word not in noise_list]\n",
    "    noise_free_text = \" \".join(noise_free_words)\n",
    "    return noise_free_text\n",
    "    \n",
    "_remove_noise(\"this is my text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to use the regular expressions while dealing with special patterns of noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove this  from the sentence'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def _remove_regex(input_text, regex_pattern):\n",
    "    urls = re.finditer(regex_pattern, input_text)\n",
    "    for i in urls:\n",
    "        input_text = re.sub(i.group().strip(), '', input_text)\n",
    "    return input_text\n",
    "\n",
    "regex_pattern = \"#[\\w]*\"\n",
    "\n",
    "_remove_regex(\"remove this #hashtag from the sentence\", regex_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Lexicon Normalization\n",
    "\n",
    "1. Stemming: Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
    "\n",
    "2. Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multiply'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"multiplying\"\n",
    "lem.lemmatize(word, \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multipli'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem.stem(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.3 Object Standardization\n",
    "\n",
    "Text data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by search engines and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retweet this is a retweeted tweet by Amit Kumar'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_dict = {'rt':'Retweet', 'dm':'direct message', 'awsm':'awesome',\n",
    "              'luv':'love'}\n",
    "def _lookup_words(input_text):\n",
    "    words = input_text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word)\n",
    "        new_text = \" \".join(new_words)\n",
    "    return new_text\n",
    "\n",
    "_lookup_words(\"RT this is a retweeted tweet by Amit Kumar\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 POS Tagging\n",
    "\n",
    "The pos tags defines the usage and function of a word in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('in', 'IN'), ('Data-Science', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "text = \"I am learning Natural Language Processing in Data-Science\"\n",
    "tokens = word_tokenize(text)\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Entity Extraction (Entities as features)\n",
    "\n",
    "### Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running and Training LDA model on the documnt-term-matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics = 3, \n",
    "              id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.060*\"driving\" + 0.060*\"pressure.\" + 0.060*\"that\" + 0.060*\"Doctors\" + 0.060*\"and\" + 0.060*\"may\" + 0.060*\"stress\" + 0.060*\"suggest\" + 0.060*\"increased\" + 0.060*\"cause\"'), (1, '0.089*\"to\" + 0.051*\"my\" + 0.051*\"sister\" + 0.051*\"My\" + 0.051*\"not\" + 0.051*\"likes\" + 0.051*\"have\" + 0.051*\"bad\" + 0.051*\"consume.\" + 0.051*\"is\"'), (2, '0.053*\"driving\" + 0.053*\"sister\" + 0.053*\"My\" + 0.053*\"my\" + 0.053*\"father\" + 0.053*\"around\" + 0.053*\"a\" + 0.053*\"lot\" + 0.053*\"time\" + 0.053*\"spends\"')]\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "\n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'is'], ['is', 'a'], ['a', 'simple'], ['simple', 'text']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    output = []\n",
    "    for i in range(len(words)-n+1):\n",
    "        output.append(words[i:i+n])\n",
    "    return output\n",
    "\n",
    "generate_ngrams('This is a simple text', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.34520501686496574\n",
      "  (0, 4)\t0.444514311537431\n",
      "  (0, 2)\t0.5844829010200651\n",
      "  (0, 7)\t0.5844829010200651\n",
      "  (1, 3)\t0.652490884512534\n",
      "  (1, 0)\t0.652490884512534\n",
      "  (1, 1)\t0.3853716274664007\n",
      "  (2, 5)\t0.5844829010200651\n",
      "  (2, 6)\t0.5844829010200651\n",
      "  (2, 1)\t0.34520501686496574\n",
      "  (2, 4)\t0.444514311537431\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "X = obj.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['data', 'science'], ['vidhya', 'science', 'data', 'analytics'],\n",
    "            ['machine', 'learning'], ['deep', 'learning']]\n",
    "\n",
    "# Train the model on your corpus\n",
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\my\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.10650278"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('data', 'science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\my\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 4.7074642e-04, -4.0449761e-03,  4.4476958e-03,  4.9279034e-03,\n",
       "        3.3356827e-03,  4.9486686e-03,  2.4012255e-03, -1.4502240e-03,\n",
       "        3.8642315e-03, -5.9284244e-05, -9.9866789e-05,  3.2409059e-03,\n",
       "        4.7122361e-03,  3.4542959e-03, -3.9117397e-03,  3.2164054e-03,\n",
       "        2.6465210e-03,  2.2529254e-03, -4.7050547e-03, -4.4319904e-03,\n",
       "        2.3893011e-03, -2.5542651e-03, -9.5354568e-04, -1.5621400e-03,\n",
       "       -4.7711013e-03,  2.1777414e-03,  4.7961813e-03,  4.1071028e-03,\n",
       "        3.7132013e-03,  7.4373034e-04,  4.1487932e-04, -1.4916781e-03,\n",
       "       -6.3188130e-04, -1.1342270e-03, -2.5184678e-03, -3.2168392e-03,\n",
       "        3.9951233e-04, -1.6535937e-03, -2.1824467e-03, -6.7424844e-04,\n",
       "       -3.9499383e-03, -4.3253209e-03, -3.1940225e-03,  3.4526286e-03,\n",
       "        4.3719881e-03, -3.1065263e-03,  2.4647971e-03, -5.3021009e-04,\n",
       "       -1.8466379e-03,  4.2802030e-03,  3.3262454e-03,  3.3525510e-03,\n",
       "       -1.4499974e-03, -2.1693400e-04,  4.3362179e-03, -4.3266122e-03,\n",
       "        1.9388410e-03,  4.3694228e-03, -1.7531108e-03,  3.0508586e-03,\n",
       "       -2.9541173e-03, -3.4418501e-04, -1.0888267e-03, -7.5621775e-04,\n",
       "       -5.1561120e-04, -4.0646228e-03,  1.1021738e-03, -4.8885061e-03,\n",
       "        2.1336467e-03,  2.4669969e-03,  4.4214274e-03,  2.0213139e-03,\n",
       "       -2.8487206e-03, -2.0542550e-03, -2.4218061e-03,  1.0592178e-03,\n",
       "        4.3971138e-03, -4.7059227e-03, -1.4132245e-03, -4.3324623e-03,\n",
       "        2.2298961e-03, -1.3716321e-04,  9.8118023e-04,  3.5961280e-03,\n",
       "       -1.5995367e-03,  3.4667836e-03, -9.0127817e-04, -4.2654732e-03,\n",
       "        4.1005356e-04,  4.5648501e-03, -1.9480478e-03, -3.5913158e-03,\n",
       "       -3.9431495e-03,  1.3461678e-03, -1.2939922e-03, -2.2689325e-03,\n",
       "       -1.8799768e-03, -5.7819329e-04, -3.0240163e-03,  3.9384556e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
